{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a099aecc-fb15-4b95-9817-88b02b0013a3",
   "metadata": {},
   "source": [
    "Data comes in a variety of formats that is more than a standard spreadsheet, for example social network and text data. Although this can be considered unstructured data, often this data is structured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901baf0-c666-4504-8d07-bb7136a4e3d5",
   "metadata": {},
   "source": [
    "# Machine learning concepts\n",
    "\n",
    "Review concepts as  what data structured data means. Cover supervised learning, overfitting, curse of dimensionality (from a geometric perspective). Additionally, this chapter covers data types-- network data, image data, and text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dda181-a866-4082-8499-f7301ef1a829",
   "metadata": {},
   "source": [
    "## Machine learning categories\n",
    "\n",
    "### supervised learning\n",
    "\n",
    "Often aimed at making predictions. This includes regressions and classification algorithms. Such as K-nearest neighbors, naive Bayes, support vector machines, random forests, gradient boosting, and neural networks. It is important to know the nature of a function, such as nonparametric functions, or functions that are computed algorithmically. Supervised learning often splits the data into a training and test set which is used to measures the effectiveness of the coefficients in predicting a variable. Once trained, you can estimate the impact of each independent variable (feature importance) which can help identify insights for decision making.\n",
    "\n",
    "### unsupervised learning\n",
    "\n",
    "Often used for data exploration, such as reducing the dimensionality of a data set, identify relationships between data, and identify outliers. Unlike supervised learning, this method only has independent variables. Therefore, we do not need to split the data. This method can be used for market segmentation or for visualizing groups. Some algorithms include k-means clustering, hierarchical clustering, and principal component analysis.\n",
    "\n",
    "These techniques can be used in tandem such as using unsupervised learning to find new questions then use supervised learning to answer them, or use unsupervised learning for dimension reduction as a preprocessing step.\n",
    "\n",
    "### Matching algorithms and other machine learning\n",
    "\n",
    "Algorithms that match the distance between data points. Useful for recommendation systems or data integrity checks (checks if there is sufficient data within a population). This data integrity is useful to prevent over-fitting to the majority population. You can also use this when you cant have randomized controlled trials.\n",
    "\n",
    "## Structured Data\n",
    "\n",
    "Can also be though of as tabular data, which allows us to use independent variables, dependent variables, and data points (observations).\n",
    "\n",
    "### The Geometry of Dummy Variables\n",
    "\n",
    "To use algorithms you will need to convert data points into an acceptable format. Such as converting categorical values, like gender, into numerical representations of those values. Or satisfaction values into a one dimensional axis, E.g. very dissatisfied to 0 and very satisfied into 5. We want to avoid creating a one dimensional on non-ordered categories to prevent artificially imposing an order. For example, if we have male, female, and non binary (3 values) on an axis then male would be farther from non binary (2). There for people either create bools for each value, or bools for all but one. By having 3 bools, we place equal distance between each category-- Similar to having an equilateral triangle where each category as vertices. However, when we transpose this equilateral triangle by having all but one, we go from having a three-dimensional equilateral triangle into a two dimensional right triangle.\n",
    "However, by using n-1 we dont run into the issue of multicollinearity which impacts linear and logistic regressions. Additionally, this reduces the curse of dimensionality. \n",
    "However, this is not true for other algorithms such as k-NNs where distance is crucial. In this case, dropping a variable will skew the results and lead to subpar performance.\n",
    "\n",
    "## THe Geometry of Numerical spreadsheets\n",
    "\n",
    "A spreadsheet can be viewed as a collection of data points in a euclidean vector space (R^d) where d is the number of columns. R^1 is a line, R^2 is a plane, and R^3 is a three-dimensional space. Anything higher is hard for people to imagine. By viewing things in a euclidean vector space we can compute things such as the distance between any pair of data points. This things leads to things such as SMOTE, which cna be used to adjust samples with imbalanced, or computing the mean of each coordinate to impute missing data. However, this method makes an assumption the euclidean space has a standard shape or structure. For example, converting geospatial data which is a sphere into a flat space distorts the distance between two points. Therefore, its important to know when to use Euclidean distance and spherical geometry in this example. \n",
    "\n",
    "You will need to do 1 or more of the follow:\n",
    "\n",
    "- Apply versions of hte usual machine learning algorithms that have been adapted ot more general geometric setting\n",
    "- Applying new geometrically powered algorithms that are based on the shape of data\n",
    "- Providing meaningful global coordinates to transform your data into a structured spreadsheet in a way that traditional statistical and machine learning tools can be successfully applied\n",
    "\n",
    "## The geometry of supervised learning\n",
    "\n",
    "Covers: classification, regression, over-fitting, and curse of dimensionality.\n",
    "\n",
    "### Classification\n",
    "\n",
    "After converting the dataset into a numerical spreadsheet,a classifier can be used to \n",
    "predict what class a data point belongs to (decision boundaries).\n",
    "\n",
    "Logistic classifier: linear decision boundaries, non-linear can be achieved by adding higher-order terms\n",
    "Decision trees classifier: splits independent variables with individual inequalities which results in a decision boundary. Contains horizontal and vertical lines.\n",
    "Random forest classifier: Similar to decision trees, but involves curved-looking shapes that are made of smaller horizontal and vertical segments.\n",
    "K-nearest neighbor (K-nn): produces polygonal decision boundaries based on the space based on which of the finitely train points are closest.\n",
    "neural nets: complex and curving decision boundaries, can lead to overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b56f78-595d-4aaa-90cd-b0e829c511ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
